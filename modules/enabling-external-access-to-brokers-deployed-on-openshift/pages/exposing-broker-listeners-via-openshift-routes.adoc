#  Exposing Broker Listeners via OpenShift Routes

= Exposing Broker Listeners via OpenShift Routes

Asynchronous messaging often requires clients to connect to the AMQ Broker from outside the OpenShift cluster. OpenShift's networking capabilities, specifically Services and Routes, provide the mechanism to expose these broker listeners (known as *acceptors*) securely and efficiently. The AMQ Broker Operator automates much of this process, creating the necessary network resources when an acceptor is configured for external access.

== Detailed Technical Explanation

When you configure an acceptor on your AMQ Broker instance for external access, the AMQ Broker Operator plays a crucial role. It automatically provisions the necessary OpenShift networking components to make the broker accessible.

=== Automatic Route Creation

For each broker pod within your deployment, the Operator automatically creates a dedicated OpenShift `Service` and `Route` corresponding to each exposed acceptor. This route acts as an entry point for external clients.

The full host name of the route is typically constructed by the Operator and includes elements like the broker deployment name, pod index, service type, OpenShift project name, and the OpenShift cluster's domain. External clients use this full host name to establish connections.

For example, a `curl` command to test external access might look like this:

[source,bash]
----
$ curl https://my-broker-deployment-0-svc-rte-my-openshift-project.my-openshift-domain
----

This route host name must resolve to the node that is hosting the OpenShift router, which then uses the host name to determine where to send the traffic inside the OpenShift internal network.

=== OpenShift Router Behavior

The OpenShift router handles both secured and non-secured traffic by default:

*   It listens on port `80` for non-secured (HTTP) traffic.
*   It listens on port `443` for secured (HTTPS/SSL-encrypted) traffic.

When an external client attempts to connect, the router automatically directs traffic to port `443` if a secure connection URL (e.g., `https://...`) is specified, or to port `80` for non-secure connection URLs (e.g., `http://...`).

=== Customizing Route Host Names

You can customize the host name of the routes exposed for your broker's acceptors to align with your organization's internal routing configuration. This is achieved by modifying the `ActiveMQArtemis` Custom Resource (CR) instance:

*   *Using `ingressHost`:* This attribute allows you to replace the default host name generated by the Operator with a specific custom host name for a particular acceptor.
*   *Using `ingressDomain`:* This attribute appends a custom domain to the host name. This custom domain is applied not only to the acceptor routes but also to all other routes exposed by the CR configuration, such as the management console route.

These attributes are typically configured within the `spec` section of your `ActiveMQArtemis` CR.

=== Load Balancing for External Clients

For scenarios where external clients need to load balance connections across multiple brokers in a cluster, you can configure load balancing at the OpenShift Route level.

To enable load balancing:

.   Add the `haproxy.router.openshift.io/balance: roundrobin` annotation to the OpenShift Route for each broker pod. This instructs the OpenShift router to distribute incoming connections using a round-robin strategy.

.   If external clients are using the AMQ Core protocol, you must also set the `useTopologyForLoadBalancing=false` key in the clientâ€™s connection URL. This prevents the Core client from attempting its own topology-aware load balancing, allowing the OpenShift router's load balancing to take precedence.

== Hands-on Activity: Exposing and Testing Broker Listeners

This lab guides you through verifying the exposure of broker listeners and testing external client connectivity.

=== Prerequisites

*   An OpenShift cluster with the AMQ Broker Operator installed.
*   An `ActiveMQArtemis` broker instance already deployed. For this lab, we'll assume a basic broker named `my-broker-deployment`.
*   `oc` CLI tool configured and logged into your OpenShift cluster.

=== Procedure

.Verify Broker Deployment and Acceptors
Before proceeding, ensure your AMQ Broker is deployed and running, and that it has at least one acceptor defined in its `ActiveMQArtemis` CR. By default, the Operator creates several standard acceptors (e.g., AMQP, Core, MQTT).

a.  Get the `ActiveMQArtemis` CR for your broker:
+
[source,bash]
----
oc get activemqartemises.broker.amq.io my-broker-deployment -o yaml
----
+
Review the output, specifically the `spec.acceptors` section, to confirm that acceptors are configured. For example:
+
[source,yaml]
----
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: my-broker-deployment
spec:
  # ... other configurations
  acceptors:
    - name: amqp
      port: 5672
      protocols: AMQP
    - name: amqp-ssl
      port: 5671
      protocols: AMQP
      sslEnabled: true
      # ... other SSL configuration (certificate, secretRef)
  # ...
----

.Identify the Exposed Routes
The Operator automatically creates routes for exposed acceptors. Let's find them.

a.  List the routes in your OpenShift project that belong to your broker deployment:
+
[source,bash]
----
oc get routes -l activemqartemis.broker.amq.io/deployment-name=my-broker-deployment
----
+
You should see routes similar to this output (names and domains will vary based on your cluster):
+
[source,text]
----
NAME                                     HOST/PORT                                                                                PATH   SERVICES                                 PORT      TERMINATION          WILDCARD
my-broker-deployment-amqp-0-svc-rte      my-broker-deployment-amqp-0-svc-rte-myproject.apps.cluster.example.com                   amqp-0   my-broker-deployment-amqp-0-svc-rte      <all>     passthrough          None
my-broker-deployment-amqp-ssl-0-svc-rte  my-broker-deployment-amqp-ssl-0-svc-rte-myproject.apps.cluster.example.com               amqp-0   my-broker-deployment-amqp-ssl-0-svc-rte  <all>     passthrough          None
my-broker-deployment-console-0-svc-rte   my-broker-deployment-console-0-svc-rte-myproject.apps.cluster.example.com                console-0   my-broker-deployment-console-0-svc-rte  <all>     passthrough          None
----
+
Note down the `HOST/PORT` for one of your SSL-enabled AMQP routes (e.g., `my-broker-deployment-amqp-ssl-0-svc-rte-myproject.apps.cluster.example.com`). This is the external hostname clients will use.

.Test External Connectivity using `curl`
While AMQ Broker primarily uses binary messaging protocols (like AMQP, Core, MQTT), `curl` can be used to perform a basic HTTP GET request to verify that the route is reachable over HTTPS for SSL-enabled routes. This helps confirm network reachability, even if the protocol doesn't match.

a.  Use `curl` with the route host name you identified:
+
[source,bash]
----
curl -v https://<YOUR_AMQP_SSL_ROUTE_HOSTNAME>
----
+
Replace `<YOUR_AMQP_SSL_ROUTE_HOSTNAME>` with the actual host name you noted in the previous step.
+
You might receive an HTTP 503 error, a connection refused, or a protocol error message. This is expected since the route is designed for a messaging protocol, not HTTP. However, if the connection attempt starts and eventually fails with a protocol-level error (rather than "couldn't resolve host" or "connection refused immediately"), it indicates that the OpenShift Route is correctly directing traffic to the broker pod.

. (Optional) Customize Route Hostname using `ingressDomain`
You can customize the domain portion of the hostname for all exposed routes.

a.  Edit the `ActiveMQArtemis` CR for `my-broker-deployment`:
+
[source,bash]
----
oc edit activemqartemises.broker.amq.io my-broker-deployment
----

b.  Add or modify the `ingressDomain` attribute in the `spec` section, for example:
+
[source,yaml]
----
spec:
  # ... other configurations
  ingressDomain: custom-apps.example.com
  # ...
----
+
Save and exit the editor. The Operator will reconcile the changes.

c.  After a few moments, check the routes again:
+
[source,bash]
----
oc get routes -l activemqartemis.broker.amq.io/deployment-name=my-broker-deployment
----
+
Observe how the `HOST/PORT` for the routes has updated to include your `custom-apps.example.com` domain.

. (Optional) Configure Load Balancing for External Access
If you have a multi-pod broker deployment and wish for external clients to load balance across pods via the OpenShift router.

a.  Identify the names of the routes created for your broker pods. For a 2-pod deployment, you'd typically have distinct routes for pod-0 and pod-1.
+
[source,bash]
----
oc get routes -l activemqartemis.broker.amq.io/deployment-name=my-broker-deployment -o name
----
+
This will list routes like `route.route.openshift.io/my-broker-deployment-amqp-0-svc-rte` and `route.route.openshift.io/my-broker-deployment-amqp-1-svc-rte`.

b.  Edit each specific route to add the load balancing annotation. For example, for the AMQP route for pod-0:
+
[source,bash]
----
oc edit route my-broker-deployment-amqp-0-svc-rte
----

c.  Add the `haproxy.router.openshift.io/balance` annotation under `metadata.annotations`:
+
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/balance: roundrobin
    # ... potentially other annotations
  name: my-broker-deployment-amqp-0-svc-rte
  # ...
spec:
  # ...
----
+
Save and exit. Repeat this for all other routes (e.g., `my-broker-deployment-amqp-1-svc-rte`) that you want to include in the load balancing.

d.  If using AMQ Core protocol clients with this load balancing setup, remember to configure `useTopologyForLoadBalancing=false` in the client's connection URL. This client-side setting works in conjunction with the OpenShift Route load balancing to ensure proper distribution of connections.