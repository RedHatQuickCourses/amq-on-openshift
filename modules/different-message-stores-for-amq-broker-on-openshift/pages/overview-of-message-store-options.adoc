= Overview of Message Store Options

Message persistence is a fundamental aspect of reliable message-oriented middleware (MOM) systems like Red Hat AMQ Broker. It ensures that messages are not lost due to broker restarts, system failures, or network outages. The *message store* is the component responsible for durably saving messages and broker state to disk.

AMQ Broker, based on Apache ActiveMQ Artemis, offers flexible options for its message store, primarily leveraging a high-performance file-based journal by default, with an alternative option to use a JDBC-compliant database. Understanding these options is crucial for designing a robust and performant messaging infrastructure, especially when deploying on OpenShift.

== Core Concepts of Message Persistence

Before diving into specific store types, let's clarify the key directories and their roles in AMQ Broker's persistence mechanism. These directories are typically located within a persistent volume mounted to the broker pod when deployed on OpenShift.

*   *Journal Directory (`journalDirectory`)*: This is the primary message store. It acts as a write-ahead log (WAL) or transaction journal, where all persistent messages, transactions, and critical broker operations (like queue creation, durable subscription bindings, acknowledgments) are recorded sequentially. Its design is optimized for high-throughput, low-latency writes.
*   *Paging Directory (`pagingDirectory`)*: This directory is used for *paging* messages to disk. When a queue accumulates a very large number of messages that exceed configurable memory limits, or when producers are significantly faster than consumers, AMQ Broker can *page* these messages out of memory and onto disk. This mechanism prevents out-of-memory errors and maintains broker stability under high load. Paged messages are reloaded into memory as consumers become available.
*   *Bindings Directory (`bindingsDirectory`)*: Stores durable bindings for queues, topics, and subscriptions. This ensures that even after a broker restart, the configuration of persistent destinations and subscriptions is preserved.
*   *Large Messages Directory (`largeMessagesDirectory`)*: Dedicated storage for messages that exceed a certain configured size threshold. Rather than storing the entire large message inline within the main journal, a reference is stored, and the message payload itself is written to this separate directory. This optimizes journal performance for smaller messages and efficient handling of very large payloads.

These directories collectively manage the persistent state of the broker and its messages.

== File-Based Message Store (Default)

The default and generally recommended message store for AMQ Broker is its high-performance *file-based journal*. This approach offers superior performance characteristics due to its optimized append-only write strategy and minimal overhead.

=== How it Works

The file-based journal operates like a database transaction log. Messages and other persistent data are written sequentially to a series of fixed-size data files within the `journalDirectory`. This sequential I/O is extremely fast, especially when backed by modern SSD storage.

*   When a persistent message arrives, it's appended to the journal.
*   When a message is consumed and acknowledged, a corresponding acknowledgment record is written to the journal.
*   The broker periodically compacts and cleans up journal files once all relevant data within them has been processed or acknowledged.

=== Advantages

*   *High Performance*: Optimized for sequential disk writes, leading to very low latency and high throughput for persistent messaging.
*   *Robustness*: Designed for crash recovery, ensuring data integrity even in the event of unexpected broker termination.
*   *Simplicity*: Easy to configure and manage, as it requires only a local filesystem path.

=== OpenShift Considerations for File-Based Store

When deploying AMQ Broker on OpenShift, using the file-based message store necessitates the use of *Persistent Volumes (PVs)* and *Persistent Volume Claims (PVCs)*. The `journalDirectory`, `pagingDirectory`, `bindingsDirectory`, and `largeMessagesDirectory` must be mapped to persistent storage to ensure message data persists beyond the lifespan of individual broker pods.

.Example: Broker CR fields for File-Based Storage
[source,yaml]
----
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: my-broker
spec:
  deploymentPlan:
    size: 1
    storage:
      size: 10Gi
      selector: {} # Optional: label selector for PV
      # journalDirectory: /var/lib/artemis/data/journal # These are default paths within the pod
      # pagingDirectory: /var/lib/artemis/data/paging
      # bindingsDirectory: /var/lib/artemis/data/bindings
      # largeMessagesDirectory: /var/lib/artemis/data/largemessages
----
In the `storage` section of the `ActiveMQArtemis` Custom Resource (CR), you specify the desired size for the Persistent Volume. The AMQ Broker Operator then provisions the necessary PV/PVCs and mounts them to the broker pods at the default paths (`/var/lib/artemis/data/journal`, etc.) within the container.

== JDBC Message Store

As an alternative to the file-based journal, AMQ Broker can be configured to use a *JDBC-compliant relational database* as its message store. This option allows you to leverage existing database infrastructure and integrate message persistence with your organization's data management strategies.

=== How it Works

When configured for JDBC persistence, AMQ Broker uses a database to store:

*   Persistent messages (typically in a messages table).
*   Queue and subscription bindings (in a bindings table).
*   Transaction metadata.

The broker interacts with the database via standard JDBC drivers.

=== Advantages

*   *Centralized Data Management*: Leverages existing database backup, replication, and management tools.
*   *Familiarity*: For organizations with strong database expertise, managing the message store through standard SQL tools might be preferred.
*   *Integration*: Easier to integrate with other database-centric systems or monitoring solutions already in place.

=== Disadvantages

*   *Performance Overhead*: Typically, JDBC persistence is slower than the file-based journal due to the overhead of database transactions, network latency (if the DB is remote), and the inherent complexity of relational databases compared to a simple write-ahead log.
*   *Database Management*: Requires an external database instance to be provisioned, managed, and maintained.
*   *Configuration Complexity*: More complex to configure, requiring datasource definitions, driver management, and potentially schema setup.

=== OpenShift Considerations for JDBC Store

Deploying with a JDBC message store on OpenShift involves:

1.  **Database Provisioning**: An external database (e.g., PostgreSQL, MySQL, Oracle, SQL Server) must be available and accessible from the OpenShift cluster. This could be an OpenShift-managed database (e.g., using a Database-as-a-Service operator) or an external database instance.
2.  **JDBC Driver**: The appropriate JDBC driver must be made available to the broker pods. This is usually achieved by mounting it into the broker's classpath.
3.  **Datasource Configuration**: The `ActiveMQArtemis` CR needs to be configured with the JDBC connection URL, credentials, and driver details.

.Example: Broker CR fields for JDBC Storage (Conceptual)
[source,yaml]
----
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: my-jdbc-broker
spec:
  deploymentPlan:
    size: 1
    persistence:
      config:
        type: JDBC # Indicate JDBC persistence
        jdbcStore:
          jdbcConnectionUrl: jdbc:postgresql://my-db-service:5432/artemis
          jdbcDriverClassName: org.postgresql.Driver
          # ... other JDBC properties like user, password (via secretRef)
          # ... and table names configuration
----
*Note:* The actual `ActiveMQArtemis` CR structure for JDBC persistence might vary slightly based on the AMQ Broker Operator version and specific configuration options. This is a conceptual representation.

== Journal vs. Paging for Message Persistence

It's important to differentiate the distinct roles of the journal and paging within the AMQ Broker's persistence architecture:

*   *Journal*: The primary, authoritative store for *all persistent data*. It's a critical component for durability and crash recovery. All persistent messages *initially* go into the journal.
*   *Paging*: A secondary mechanism for handling *overflow*. When queues become too large, messages are moved from memory (where they might have arrived from the journal) to the paging directory on disk. This is a *flow control* mechanism to protect the broker's memory resources by offloading messages to disk until consumers are ready.

Both mechanisms work in concert to provide robust message persistence and ensure broker stability under varying load conditions.

== Choosing the Appropriate Message Store Strategy

The choice between a file-based and JDBC message store depends on several factors:

*   *Performance Requirements*: For maximum throughput and lowest latency, the file-based journal is generally superior.
*   *Operational Expertise*: If your team has strong database administration skills and a preference for managing data in a relational database, JDBC might be considered.
*   *Existing Infrastructure*: Leveraging an existing, well-managed database might be appealing.
*   *Complexity*: File-based is simpler to set up and manage, especially with the AMQ Broker Operator handling Persistent Volume provisioning. JDBC introduces external database dependency and configuration.
*   *High Availability (HA) Strategy*: Both can support HA, but the implementation details differ (e.g., shared storage for file-based HA vs. database HA for JDBC).

For most OpenShift deployments of AMQ Broker, the *file-based message store backed by Persistent Volumes is the recommended and default choice* due to its performance benefits and seamless integration with OpenShift's storage capabilities managed by the Operator.

== Hands-on Activity: Observing Default Message Store Setup

This activity helps you verify that the default file-based message store is provisioned and used correctly by the AMQ Broker Operator on OpenShift.

.Procedure: Inspecting Default Storage
.  **Deploy a Basic Broker**: If you haven't already, deploy a basic AMQ Broker instance using the Operator.
+
[IMPORTANT]
.Wait for the Operator
Make sure the AMQ Broker Operator is installed and running in your cluster before executing this command.
+
[source,bash]
----
oc new-project amq-broker-test
oc project amq-broker-test
oc apply -f - <<EOF
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: my-broker
spec:
  deploymentPlan:
    size: 1
    storage:
      size: 1Gi # Small size for quick testing
EOF
----
.  **Verify Persistent Volume Claim (PVC)**: After the broker pod starts, a PVC will be created and bound.
+
[source,bash]
----
oc get pvc -n amq-broker-test
----
+
You should see a PVC similar to `my-broker-data-my-broker-ss-0` with a status of `Bound`.
.  **Access the Broker Pod**: Connect to the running broker pod using `oc rsh`.
+
[source,bash]
----
oc rsh -n amq-broker-test my-broker-ss-0
----
.  **Inspect Storage Directories**: Once inside the pod, navigate to the default data directory `/var/lib/artemis/data` and observe the persistence files.
+
[source,bash]
----
ls -l /var/lib/artemis/data/
ls -l /var/lib/artemis/data/journal
ls -l /var/lib/artemis/data/paging
ls -l /var/lib/artemis/data/bindings
ls -l /var/lib/artemis/data/largemessages
----
+
You will see various files (e.g., `activemq-data-*.amq`, `activemq-bindings-*.amq`, `activemq-journal-*.amq`, `activemq-paging-*.page`) within these directories, indicating the broker is actively using the file-based message store.
.  **Exit the Pod**: Type `exit` to leave the pod shell.

This simple activity demonstrates that by default, the AMQ Broker Operator provisions persistent storage and configures the broker to use its file-based message store for durability on OpenShift.